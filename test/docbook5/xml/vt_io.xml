<!-- Converted by db4-upgrade version 1.1 -->

<sect1 xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0" xml:id="sec.vt.io">
  <title>I/O Virtualization</title>
  <para>
  VM Guests not only share CPU and memory resources of the host system, but
  also the I/O subsystem. Because software I/O virtualization techniques
  deliver less performance than bare metal, hardware solutions that deliver
  almost <quote>native</quote> performance have been developed recently.
  <phrase role="productname"><phrase os="osuse">openSUSE</phrase><phrase os="sles">SUSE Linux Enterprise Server</phrase><phrase os="sled">SUSE Linux Enterprise Desktop</phrase><phrase os="slerte">SUSE Linux Enterprise Real Time Extension</phrase></phrase> supports the following I/O virtualization techniques:
 </para>
  <variablelist>
    <varlistentry xml:id="vt.io.fullv">
      <term>Full Virtualization</term>
      <listitem>
        <para>
     Fully Virtualized (FV) drivers emulate widely supported real devices,
     which can be used with an existing driver in the VM Guest. Since the
     physical device on the VM Host Server may differ from the emulated one, the
     hypervisor needs to process all I/O operations before handing them over
     to the physical device. Therefore all I/O operations need to traverse
     two software layers, a process that not only significantly impacts I/O
     performance, but also consumes CPU time.
    </para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="vt.io.parav">
      <term>Paravirtualization</term>
      <listitem>
        <para>
     Paravirtualization (PV) allows direct communication between the
     hypervisor and the VM Guest. With less overhead involved, performance
     is much better than with full virtualization. However,
     paravirtualization requires either the guest operating system to be
     modified to support the paravirtualization API or paravirtualized
     drivers. See
     <xref linkend="sec.kvm.requires.guests.virt_drivers"/> for a list
     of guest operating systems supporting paravirtualization.
    </para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="vt.io.pcipass">
      <term>PCI-Passthrough</term>
      <listitem>
        <para>
     Directly assigning a PCI device to a VM Guest (PCI pass-through)
     avoids performance issues caused by avoiding any emulation in
     peformance critical paths. With PCI pass-through, a VM Guest can
     directly access the real hardware using a native driver, getting almost
     native performance. This method does not allow to share
     devices—each device can only be assigned to a single VM Guest.
     PCI-Passthrough needs to be supported by the VM Host Server CPU, chipset and
     the BIOS/EFI. The VM Guest needs to be equipped with drivers for the
     device. See <xref linkend="sec.libvirt.config.pci"/> or
     <xref linkend="sec.libvirt.config.pci.virsh"/> for setup
     instructions.
    </para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="vt.io.sriov">
      <term>SR-IOV</term>
      <listitem>
        <para>
     The latest I/O virtualization technique, Single Root I/O Virtualization
     SR-IOV combines the benefits of the aforementioned
     techniques—performance and the ability to share a device with
     several VM Guests. SR-IOV requires special I/O devices, that are
     capable of replicating resources so they appear as multiple separate
     devices. Each such <quote>pseudo</quote> device can be directly used by
     a single guest. However, for network cards for example the number of
     concurrent queues that can be used is reduced, potentially reducing
     performance for the VM Guest compared to paravirtualized drivers. On
     the VM Host Server, SR-IOV must be supported by the I/O device, the CPU and
     chipset, the BIOS/EFI and the hypervisor—see
     <xref linkend="sec.libvirt.config.io"/> for setup instructions.
<!-- fs 2014-02-21: no list available
     ATM &productname; supports the SRV-IOV capable network cards listed below
     --></para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="vt.io.vfio">
      <term>VFIO</term>
      <listitem>
        <para>
     VFIO stands for <emphasis>Virtual Function I/O</emphasis>. It allows
     safe, non-privileged user space drivers. The VFIO driver is a
     device-agnostic framework to expose direct device access in a protected
     environment. Its primary purpose is to replace the KVM PCI-specific
     device assignment. For more information, see
     <xref linkend="kvm.vfio"/>.
    </para>
      </listitem>
    </varlistentry>
  </variablelist>
</sect1>
