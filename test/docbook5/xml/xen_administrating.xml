<!-- Converted by db4-upgrade version 1.1 -->

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0" xml:id="cha.xen.admin">
  <title>Administration Tasks</title>
  <para/>
  <sect1 xml:id="sec.xen.config.bootloader">
    <title>The Boot Loader Program</title>
    <remark condition="clarity">
    2014-02-10 - fs: FIXME: These entries needs to be adjusted to GRUB 2.
   </remark>
    <para>
   The boot loader controls how the virtualization software boots and runs.
   You can modify the boot loader properties by using YaST, or by directly
   editing the boot loader configuration file.
  </para>
    <para>
   The YaST boot loader program is located at <menuchoice><guimenu>YaST</guimenu><guimenu>System</guimenu><guimenu>Boot
   Loader</guimenu></menuchoice>. Click the <guimenu>Bootloader
   Options</guimenu> tab and select the line containing the Xen kernel as
   the <guimenu>Default Boot Section</guimenu>.
  </para>
    <figure>
      <title>Boot Loader Settings</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
        </imageobject>
      </mediaobject>
    </figure>
    <para>
   Confirm with <guimenu>OK</guimenu>. Next time you boot the host, it will
   be ready to provide the Xen virtualization environment.
  </para>
    <para>
   You can use the Boot Loader program to specify functionality, such as:
  </para>
    <itemizedlist>
      <listitem>
        <para>
     Pass kernel command line parameters.
    </para>
      </listitem>
      <listitem>
        <para>
     Specify the kernel image and initial RAM disk.
    </para>
      </listitem>
      <listitem>
        <para>
     Select a specific hypervisor.
    </para>
      </listitem>
      <listitem>
        <para>
     Pass additional parameters to the hypervisor. See
     <link xlink:href="http://xenbits.xen.org/docs/unstable/misc/xen-command-line.html"/>
     for their complete list.
    </para>
      </listitem>
    </itemizedlist>
    <para>
   You can customize your virtualization environment by editing the
   <filename>/etc/default/grub</filename> file. Add the following line to
   this file: <literal>GRUB_CMDLINE_XEN="&lt;boot_parameters&gt;"</literal>. Do
   not forget to run <command>grub2-mkconfig -o
   /boot/grub2/grub.cfg</command> after editing the file.
  </para>
  </sect1>
  <sect1 xml:id="sec.xen.config.sparse">
    <title>Sparse Image Files and Disk Space</title>
    <para>
   If the host’s physical disk reaches a state where it has no available
   space, a virtual machine using a virtual disk based on a sparse image
   file is unable to write to its disk. Consequently, it reports I/O errors.
  </para>
    <para>
   The Reiser file system, perceiving a corrupt disk environment,
   automatically sets the file system to read-only. If this situation
   happens, you should free up available space on the physical disk, remount
   the virtual machine’s file system, and set the file system back to
   read-write.
  </para>
    <para>
   To check the actual disk requirements of a sparse image file, use the
   command <command>du -h &lt;image file&gt;</command>.
  </para>
    <para>
   To increase the available space of a sparse image file, first increase
   the file size and then the file system.
  </para>
    <warning>
      <title>Back Up Before Resizing</title>
      <para>
    Touching the sizes of partitions or sparse files always bears the risk
    of data failure. Do not work without a backup.
   </para>
    </warning>
    <para>
   The resizing of the image file can be done online, while the VM Guest is
   running. Increase the size of a sparse image file with:
  </para>
    <screen>dd if=/dev/zero of=&lt;image file&gt; count=0 bs=1M seek=&lt;new size in MB&gt;</screen>
    <para>
   For example, to increase the file
   <filename>/var/lib/xen/images/sles/disk0</filename> to a size of 16GB,
   use the command:
  </para>
    <screen>dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 count=0 bs=1M seek=16000</screen>
    <note>
      <title>Increasing Non-Sparse Images</title>
      <para>
    It is also possible to increase the image files of devices that are not
    sparse files. However, you must know exactly where the previous image
    ends. Use the seek parameter to point to the end of the image file and
    use a command similar to the following:
   </para>
      <screen>dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 seek=8000 bs=1M count=2000</screen>
    </note>
    <para>
   Be sure to use the right seek, else data loss may happen.
  </para>
    <para>
   If the VM Guest is running during the resize operation, also resize the
   loop device that provides the image file to the VM Guest. First detect
   the correct loop device with the command:
  </para>
    <screen>losetup -j /var/lib/xen/images/sles/disk0</screen>
    <para>
   Then resize the loop device, for example <filename>/dev/loop0</filename>,
   with the following command:
  </para>
    <screen>losetup -c /dev/loop0</screen>
    <para>
   Finally check the size of the block device inside the guest system with
   the command <command>fdisk -l /dev/xvdb</command>. The device name
   depends on the actually increased device.
  </para>
    <para>
   The resizing of the file system inside the sparse file involves tools
   that are depending on the actual file system. This is described in detail
   in the Storage Administration Guide, found at
   <link xlink:href="http://www.suse.com/doc/sles11/stor_admin/data/bookinfo.html"/>.
  </para>
  </sect1>
  <sect1 xml:id="sec.xen.manage.migrate">
    <title>Migrating Xen VM Guest Systems</title>
    <para>
   With Xen it is possible to migrate a VM Guest system from one VM Host Server
   to another with almost no service interruption. This could be used for
   example to move a busy VM Guest to a VM Host Server that has stronger hardware
   or is not yet loaded. Or, if a service of a VM Host Server is required, all
   VM Guest systems running on this machine can be migrated to other
   machines in order to avoid interruption of service. These are only two
   examples—many more reasons may apply to your personal situation.
  </para>
    <para>
   Before starting, some preliminary considerations regarding the VM Host Server
   should be taken into account:
  </para>
    <itemizedlist>
      <listitem>
        <para>
     All VM Host Server systems should use a similar CPU. The frequency is not so
     important, but they should be using the same CPU family. To get more
     information about the used CPU, see <command>cat
     /proc/cpuinfo</command>.
    </para>
      </listitem>
      <listitem>
        <para>
     All resources that are used by a specific guest system must be
     available on all involved VM Host Server systems. This means the network
     bridges must be in the same subnet, and all used block devices must
     exist on both VM Host Server systems.
    </para>
      </listitem>
      <listitem>
        <para>
     Using special features like <literal>PCI Pass-Through</literal> may be
     problematic. Do not implement these when deploying for an environment
     that should migrate VM Guest systems between different VM Host Server
     systems.
    </para>
      </listitem>
      <listitem>
        <para>
     For fast migrations, a fast network is mandatory. If possible, use GB
     Ethernet and fast switches. Deploying VLAN might also help avoid
     collisions.
    </para>
      </listitem>
    </itemizedlist>
    <sect2 xml:id="sec.xen.manage.migrate.block">
      <title>Preparing Block Devices for Migrations</title>
      <para>
    The block devices needed by the VM Guest system must be available on
    all involved VM Host Server systems. This is done by implementing some kind of
    shared storage that serves as container for the root file system of the
    migrated VM Guest system. Common possibilities include:
   </para>
      <itemizedlist>
        <listitem>
          <para><literal>iSCSI</literal> can be set up to give access to the same
      block devices from different systems at the same time. For more
      information about iSCSI, see
      <link xlink:href="http://www.suse.com/doc/sles12/stor_admin/data/cha_inst_system_iscsi.html"/>.
     </para>
        </listitem>
        <listitem>
          <para><literal>NFS</literal> is a widely used root file system that can
      easily be accessed from different locations. For more information, see
      <xref linkend="cha.nfs"/>.
     </para>
        </listitem>
        <listitem>
          <para><literal>DRBD</literal> can be used if only two VM Host Server systems are
      involved. This gives some extra data security, because the used data
      is mirrored over the network. For more information, see
      <link xlink:href="http://www.suse.com/doc/sles12/book_sleha/data/cha_ha_drbd.html"/>.
     </para>
        </listitem>
        <listitem>
          <para><literal>SCSI</literal> can also be used if the available hardware
      permits shared access to the same disks.
     </para>
        </listitem>
        <listitem>
          <para><literal>NPIV</literal> is a special mode to use fibre channel disks.
      However, in this case all migration hosts must be attached to the same
      fibre channel switch. For more information about NPIV, see
      <xref linkend="sec.xen.config.disk"/>. Commonly, this works if
      the fibre channel environment supports 4 Gbit or faster connections.
     </para>
        </listitem>
      </itemizedlist>
    </sect2>
    <sect2 xml:id="sec.xen.manage.migrate.xl">
      <title>Migrating VM Guest Systems</title>
      <para>
    The actual migration of the VM Guest system is done with the command:
   </para>
      <screen>xl migrate &lt;domain_name&gt; &lt;host&gt;</screen>
      <para>
    The speed of the migration depends on how fast the memory print can be
    saved to disk, sent to the new VM Host Server and loaded there. This means
    that small VM Guest systems can be migrated faster than big systems
    with a lot of memory.
   </para>
    </sect2>
  </sect1>
  <sect1 xml:id="sec.xen.monitor">
    <title>Monitoring Xen</title>
    <para>
   For a regular operation of many virtual guests, having a possibility to
   check the sanity of all the different VM Guest systems is indispensable.
   Xen offers several tools besides the system tools to gather information
   about the system.
  </para>
    <tip>
      <title>Monitoring the VM Host Server</title>
      <para>
    Basic monitoring of the VM Host Server (I/O and CPU) is available via the
    Virtual Machine Manager. Refer to <xref linkend="cha.libvirt.admin.monitor.virt-manager"/>
    for details.
   </para>
    </tip>
    <sect2 xml:id="sec.xen.monitor.xentop">
      <title>Monitor Xen with <command>xentop</command></title>
      <para>
    The preferred terminal application to gather information about Xen
    virtual environment is <command>xentop</command>. Unfortunately, this
    tool needs a rather broad terminal, else it inserts line breaks into the
    display.
   </para>
      <para><command>xentop</command> has several command keys that can give you
    more information about the system that is monitored. Some of the more
    important are:
   </para>
      <variablelist>
        <varlistentry>
          <term>D</term>
          <listitem>
            <para>
       Change the delay between the refreshes of the screen.
      </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>N</term>
          <listitem>
            <para>
       Also display network statistics. Note, that only standard
       configurations will be displayed. If you use a special configuration
       like a routed network, no network will be displayed at all.
      </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>B</term>
          <listitem>
            <para>
       Display the respective block devices and their cumulated usage count.
      </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
    For more information about <command>xentop</command> see the manual page
    <command>man 1 xentop</command>.
   </para>
    </sect2>
    <sect2 xml:id="sec.xen.monitor.tools">
      <title>More Helpful Tools</title>
      <para>
    There are many different system tools that also help monitoring or
    debugging a running SUSE Linux Enterprise system. Many of these are covered in the
    official SUSE Linux Enterprise documentation. Especially useful for monitoring a
    virtualization environment are the following tools:
   </para>
      <variablelist>
        <varlistentry>
          <term>ip</term>
          <listitem>
            <para>
       The command line utility <command>ip</command> may be used to monitor
       arbitrary network interfaces. This is especially useful if you have
       set up a network that is routed or applied a masqueraded network. To
       monitor a network interface with the name
       <literal>alice.0</literal>, run the following command:
      </para>
            <screen>watch ip -s link show alice.0</screen>
          </listitem>
        </varlistentry>
      </variablelist>
      <variablelist>
        <varlistentry>
          <term>brctl</term>
          <listitem>
            <para>
       In a standard setup, all the Xen VM Guest systems are attached to
       a virtual network bridge. <command>brctl</command> allows you to
       determine the connection between the bridge and the virtual network
       adapter in the VM Guest system. For example, the output of
       <command>brctl show</command> may look like the following:
      </para>
            <screen>bridge name     bridge id               STP enabled     interfaces
br0             8000.000476f060cc       no              eth0
                                                        vif1.0
br1             8000.00001cb5a9e7       no              vlan22 </screen>
            <para>
       This shows that there are two virtual bridges defined on the system.
       One is connected to the physical Ethernet device
       <literal>eth0</literal>, the other one is connected to a VLAN
       interface <literal>vlan22</literal>.
      </para>
            <para>
       There is only one guest interface active in this setup,
       <literal>vif1.0</literal>. This means that the guest with ID 1 has
       an Ethernet interface <literal>eth0</literal> assigned, that is
       connected to <literal>br0</literal> in the VM Host Server.
      </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>iptables-save</term>
          <listitem>
            <para>
       Especially when using masquerade networks, or if several Ethernet
       interfaces are set up together with a firewall setup, it may be
       helpful to check the current firewall rules.
      </para>
            <para>
       The command <command>iptables</command> may be used to check all the
       different firewall settings. To list all the rules of a chain, or
       even of the complete setup, you may use the commands
       <command>iptables-save</command> or <command>iptables -S</command>.
      </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </sect2>
  </sect1>
  <sect1 xml:id="sec.xen.admin.vhostmd">
    <title>Providing Host Information for VM Guest Systems</title>
    <para>
   In a standard Xen environment, the VM Guest systems have only very
   limited information about the VM Host Server system they are running on. If a
   guest should know more about the VM Host Server it runs on,
   <systemitem>vhostmd</systemitem> can provide more information to selected
   guests. To set up your system to run <systemitem>vhostmd</systemitem>,
   proceed as follows:
  </para>
    <procedure>
      <step>
        <para>
     Install the package vhostmd on the VM Host Server.
    </para>
      </step>
      <step>
        <para>
     Edit the file <filename>/etc/vhostmd/vhostmd.conf</filename> if you
     want to add or remove <literal>metric</literal> sections from the
     configuration. However, the default works well.
    </para>
      </step>
      <step>
        <para>
     Check the validity of the <filename>vhostmd.conf</filename>
     configuration file with the command:
    </para>
        <screen>cd /etc/vhostmd
xmllint --postvalid --noout vhostmd.conf    
    </screen>
      </step>
      <step>
        <para>
     Start the vhostmd daemon with the command <command>sudo systemctl start
     vhostmd.service</command>.
    </para>
        <para>
     If vhostmd should be started automatically during start-up of the
     system, run the command:
    </para>
        <screen>sudo systemctl enable vhostmd.service</screen>
      </step>
      <step>
        <para>
     Attach the image file <filename>/dev/shm/vhostmd0</filename> to the
     VM Guest system named alice with the command:
    </para>
        <screen>xl block-attach opensuse /dev/shm/vhostmd0,,xvdb,ro</screen>
      </step>
      <step>
        <para>
     Log on the VM Guest system.
    </para>
      </step>
      <step>
        <para>
     Install the client package <systemitem>vm-dump-metrics</systemitem>.
    </para>
      </step>
      <step>
        <para>
     Run the command <command>vm-dump-metrics</command>. If you would like
     to have the result in a file, use the option <systemitem>-d
     &lt;filename&gt;</systemitem>.
    </para>
      </step>
    </procedure>
    <para>
   The result of the <systemitem>vm-dump-metrics</systemitem> is an XML
   output. The respective metric entries follow the DTD
   <filename>/etc/vhostmd/metric.dtd</filename>.
  </para>
    <para>
   For more information, see the manual pages <command>man 8
   vhostmd</command> and <filename>/usr/share/doc/vhostmd/README</filename>
   on the VM Host Server system. On the guest, see the manual page <command>man 1
   vm-dump-metrics</command>.
  </para>
  </sect1>
</chapter>
