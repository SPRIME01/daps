<!-- Converted by db4-upgrade version 1.1 -->

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0" xml:id="cha.xen.vhost">
  <title>Setting Up a Virtual Machine Host</title>
  <para>
  This section documents how to set up and use <phrase role="productname"><phrase os="osuse">openSUSE</phrase><phrase os="sles">SUSE Linux Enterprise Server</phrase><phrase os="sled">SUSE Linux Enterprise Desktop</phrase><phrase os="slerte">SUSE Linux Enterprise Real Time Extension</phrase></phrase><phrase role="productname"><phrase os="osuse">12.2</phrase><phrase os="sles;sled;slerte">12</phrase></phrase>
  as a virtual machine host.
 </para>
  <para>
  In most cases, the hardware requirements for the Dom0 are the same as
  those for the <phrase role="productname"><phrase os="osuse">openSUSE</phrase><phrase os="sles">SUSE Linux Enterprise Server</phrase><phrase os="sled">SUSE Linux Enterprise Desktop</phrase><phrase os="slerte">SUSE Linux Enterprise Real Time Extension</phrase></phrase> operating system, but additional CPU, disk,
  memory, and network resources should be added to accommodate the resource
  demands of all planned VM Guest systems.
 </para>
  <tip>
    <para>
   Remember that VM Guest systems, like physical machines, perform better
   when they run on faster processors and have access to more system memory.
  </para>
  </tip>
  <!-- hardware should be in RN now
 <para>
  The following table lists the minimum hardware requirements for running a
  typical virtualized environment. Additional requirements have to be added
  for the number and type of the respective guest systems.
 </para>
 <table id="tab.xen.vhost" frame="topbot" rowsep="1" pgwide="0">
  <title>Hardware Requirements</title>
  <tgroup cols="3">
   <colspec colnum="1" colname="1" colwidth="24*"/>
   <colspec colnum="2" colname="2" colwidth="76*"/>
   <thead>
    <row>
     <entry>
      <para>
       System Component
      </para>
     </entry>
     <entry>
      <para>
       Minimum Requirements
      </para>
     </entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry>
      <para>
       Computer
      </para>
     </entry>
     <entry>
      <para>
       Computer with Pentium II or AMD K7 450 MHz processor
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Memory
      </para>
     </entry>
     <entry>
      <para>
       512 MB of RAM for the host
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Free Disk Space
      </para>
     </entry>
     <entry>
      <para>
       7 GB of available disk space for the host.
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Optical Drive
      </para>
     </entry>
     <entry colname="2">
      <para>
       DVD-ROM Drive
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Hard Drive
      </para>
     </entry>
     <entry>
      <para>
       20 GB
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Network Device
      </para>
     </entry>
     <entry>
      <para>
       Ethernet 100 Mbps
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       IP Address
      </para>
     </entry>
     <entry>
      <itemizedlist>
       <listitem>
        <para>
         One IP address on a subnet for the host.
        </para>
       </listitem>
       <listitem>
        <para>
         One IP address on a subnet for each &vmguest;.
        </para>
       </listitem>
      </itemizedlist>
      <para/>
     </entry>
    </row>
   </tbody>
  </tgroup>
 </table>
-->
  <para>
  Xen virtualization technology is available in <phrase role="productname"><phrase os="osuse">openSUSE</phrase><phrase os="sles">SUSE Linux Enterprise Server</phrase><phrase os="sled">SUSE Linux Enterprise Desktop</phrase><phrase os="slerte">SUSE Linux Enterprise Real Time Extension</phrase></phrase> products
  based on code path 10 and later. Code path 10 products include Open
  Enterprise Server 2 Linux, <phrase role="productname"><phrase os="osuse">openSUSE</phrase><phrase os="sles">SUSE Linux Enterprise Server</phrase><phrase os="sled">SUSE Linux Enterprise Desktop</phrase><phrase os="slerte">SUSE Linux Enterprise Real Time Extension</phrase></phrase> 10, SUSE Linux Enterprise Desktop 10, and openSUSE
  10.x.
 </para>
  <para>
  The virtual machine host requires a number of software packages and their
  dependencies to be installed. To install all necessary packages, run
  YaST <guimenu>Software Management</guimenu>, select
  <menuchoice><guimenu>View</guimenu><guimenu>Patterns</guimenu></menuchoice> and choose <guimenu>Xen Virtual
  Machine Host Server</guimenu> for installation. The installation can also
  be performed with YaST using the module
  <menuchoice><guimenu>Virtualization</guimenu><guimenu>Install Hypervisor
  and Tools</guimenu></menuchoice>.
 </para>
  <para>
  After the Xen software is installed, restart the computer and, on the
  boot screen, choose the newly added option with the Xen kernel.
 </para>
  <para>
  Updates are available through your update channel. To be sure to have the
  latest updates installed, run YaST <guimenu>Online Update</guimenu>
  after the installation has finished.
 </para>
  <sect1 xml:id="sec.xen.vhost.best">
    <title>Best Practices and Suggestions</title>
    <para>
   When installing and configuring the SUSE Linux Enterprise operating system on the host,
   be aware of the following best practices and suggestions:
  </para>
    <itemizedlist>
      <listitem>
        <para>
     If the host should always run as Xen host, run YaST <menuchoice><guimenu>System</guimenu><guimenu>Boot Loader</guimenu></menuchoice>
     and activate the Xen boot entry as default boot section.
    </para>
        <itemizedlist>
          <listitem>
            <para>
       In YaST, click <guimenu>System &gt; Boot Loader</guimenu>.
      </para>
          </listitem>
          <listitem>
            <para>
       Change the default boot to the <guimenu>Xen</guimenu> label, then
       click <guimenu>Set as Default</guimenu>.
      </para>
          </listitem>
          <listitem>
            <para>
       Click <guimenu>Finish</guimenu>.
      </para>
          </listitem>
        </itemizedlist>
      </listitem>
      <listitem>
        <para>
     For best performance, only the applications and processes required for
     virtualization should be installed on the virtual machine host.
    </para>
      </listitem>
      <listitem>
        <!-- TODO: this is outdated!
      - document Xen with openAIS, outline with old heartbeat is
         http://www.novell.com/coolsolutions/feature/19571.html
    -->
        <para>
     When using both iSCSI and OCFS2 to host Xen images, the latency
     required for OCFS2 default timeouts in SUSE Linux Enterprise Server 12 may not be met. To
     reconfigure this timeout, run <command>systemctl configure o2cb.service
     </command> or edit <literal>O2CB_HEARTBEAT_THRESHOLD</literal> in the
     system configuration.
    </para>
      </listitem>
    </itemizedlist>
    <note>
      <!-- Bugzilla #878036 -->
      <title>Hardware Monitoring</title>
      <para>
    The Dom0 Kernel is running virtualized, so tools like
    <command>irqbalance</command> or <command>lscpu</command> will not
    reflect the real hardware characteristics.
   </para>
    </note>
  </sect1>
  <sect1 xml:id="sec.xen.vhost.memory">
    <title>Managing Dom0 Memory</title>
    <para>
   When the host is set up, a percentage of system memory is reserved for
   the hypervisor, and all remaining memory is automatically allocated to
   Dom0.
  </para>
    <para>
   A better solution is to set a default amount of memory for Dom0, so the
   memory can be allocated appropriately to the hypervisor. An adequate
   amount would be 20 percent of the total system memory up to 2 GiB. A
   recommended minimum amount would be 512 MiB
  </para>
    <warning>
      <title>Minimum amount of Memory</title>
      <para>
    The minimum amount of memory heavily depends on how many VM Guest(s)
    the host should handle. So be sure you have enough memory to support all
    your VM Guests.
   </para>
    </warning>
    <sect2 xml:id="sec.xen.vhost.maxmem">
      <title>Setting a Maximum Amount of Memory</title>
      <procedure>
        <step>
          <para>
      Determine the amount of memory to set for Dom0.
     </para>
        </step>
        <step>
          <para>
      At Dom0, type <command>xl info</command> to view the amount of
      memory that is available on the machine. The memory that is currently
      allocated by Dom0 can be determined with the command <command>xl
      list</command>.
     </para>
        </step>
        <step>
          <para>
      Run <menuchoice><guimenu>YaST</guimenu><guimenu>Boot
      Loader</guimenu></menuchoice>.
     </para>
        </step>
        <step>
          <para>
      Select the Xen section.
     </para>
        </step>
        <step>
          <para>
      In <guimenu>Additional Xen Hypervisor Parameters</guimenu>, add
      <command>dom0_mem=<replaceable>mem_amount</replaceable></command>
      where <replaceable>mem_amount</replaceable> is the maximum amount of
      memory to allocate to Dom0. Add <command>K</command>,
      <command>M</command>, or <command>G</command>, to specify the size,
      for example, <command>dom0_mem=768M</command>.
     </para>
        </step>
        <step>
          <para>
      Restart the computer to apply the changes.
     </para>
        </step>
      </procedure>
      <warning>
        <title>Xen Dom0 Memory</title>
        <para>
     When using the XL toolstack and the <command>dom0_mem=</command> option
     for the Xen hypervisor in GRUB 2 you need to disable xl
     <emphasis>autoballoon</emphasis> in <filename>etc/xen/xl.conf</filename>,
     otherwise launching VMs will fail with errors about not being able to
     balloon down Dom0. So add <emphasis>autoballoon=0</emphasis> to
     <filename>xl.conf</filename> if you have the <command>dom0_mem=</command>
     option specified for Xen. Also see <link xlink:href="http://wiki.xen.org/wiki/Xen_Best_Practices#Xen_dom0_dedicated_memory_and_preventing_dom0_memory_ballooning">http://wiki.xen.org/wiki/Xen_Best_Practices#Xen_dom0_dedicated_memory_and_preventing_dom0_memory_ballooning</link></para>
      </warning>
    </sect2>
  </sect1>
  <sect1 xml:id="sec.xen.vhost.netcard">
    <title>Network Card in Fully Virtualized Guests</title>
    <para>
   In a fully virtualized guest, the default network card is an emulated
   Realtek network card. However, it also possible to use the split network
   driver to run the communication between Dom0 and a VM Guest. By
   default, both interfaces are presented to the VM Guest, because the
   drivers of some operating systems require both to be present.
  </para>
    <para>
   When using SUSE Linux Enterprise, only the paravirtualized network cards are available
   for the VM Guest by default. The following network options are
   available:
  </para>
    <variablelist>
      <varlistentry>
        <term>emulated</term>
        <listitem>
          <para>
      To use an emulated network interface like an emulated Realtek card,
      specify <literal>type=ioemu</literal> in the <literal>vif</literal>
      device section of the domain xl configuration. An example
      configuration would look like:
     </para>
          <screen>vif = [ 'type=ioemu,mac=00:16:3e:5f:48:e4,bridge=br0' ]</screen>
          <para>
      Find more details about the xl configuration in the
      <filename>xl.conf</filename> manual page <command>man 5
      xl.conf</command>.
     </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>paravirtualized</term>
        <listitem>
          <para>
      When you specify <literal>type=vif</literal> and do not specify a model
      or type, the paravirtualized network interface is used:
     </para>
          <screen>vif = [ 'type=vif,mac=00:16:3e:5f:48:e4,bridge=br0,backen=0' ]</screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>emulated and paravirtualized</term>
        <listitem>
          <para>
      If the administrator should be offered both options, simply specify
      both type and model. The xl configuration would look like:
     </para>
          <screen>vif = [ 'type=ioemu,mac=00:16:3e:5f:48:e4,model=rtl8139,bridge=br0' ]</screen>
          <para>
      In this case, one of the network interfaces should be disabled on the
      VM Guest.
     </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </sect1>
  <sect1 xml:id="sec.xen.vhost.start">
    <title>Starting the Virtual Machine Host</title>
    <para>
   If virtualization software is correctly installed, the computer boots to
   display the GRUB 2 boot loader with a <citetitle>Xen</citetitle> option
   on the menu. Select this option to start the virtual machine host.
  </para>
    <note>
      <title>Xen and Kdump</title>
      <para>
    In Xen, the hypervisor manages the memory resource. If you need to
    reserve system memory for a recovery kernel in Dom0, this memory has
    to be reserved by the hypervisor. Thus, it is necessary to add the
    parameter <systemitem>crashkernel=size@offset</systemitem> to the
    <literal>kernel</literal> line instead of using the line with the other
    boot options.
   </para>
    </note>
    <para>
   If the <citetitle>Xen</citetitle> option is not on the GRUB 2 menu,
   review the steps for installation and verify that the GRUB 2 boot loader
   has been updated. If the installation has been done without selecting the
   Xen pattern, run the YaST <guimenu>Software Management</guimenu>,
   select the filter <guimenu>Patterns</guimenu> and choose <guimenu>Xen
   Virtual Machine Host Server</guimenu> for installation.
  </para>
    <para>
   After booting the hypervisor, the Dom0 virtual machine starts and
   displays its graphical desktop environment. If you did not install a
   graphical desktop, the command line environment appears.
  </para>
    <tip>
      <title>Graphics Problems</title>
      <para>
    Sometimes it may happen that the graphics system does not work properly.
    In this case, add <literal>vga=ask</literal> to the boot parameters. To
    activate permanent settings, use <literal>vga=mode-0x???</literal> where
    <literal>???</literal> is calculated as <literal>0x100</literal> + VESA
    mode from
    <link xlink:href="http://en.wikipedia.org/wiki/VESA_BIOS_Extensions"/>,
    e.g. <literal>vga=mode-0x361</literal>.
   </para>
    </tip>
    <para>
   Before starting to install virtual guests, make sure that the system time
   is correct. To do this, configure NTP (Network Time Protocol) on the
   controlling domain:
  </para>
    <procedure>
      <step>
        <para>
     In YaST select <menuchoice><guimenu>Network Services</guimenu><guimenu>NTP Configuration</guimenu></menuchoice>.
    </para>
      </step>
      <step>
        <para>
     Select the option to automatically start the NTP daemon during boot.
     Provide the IP address of an existing NTP time server, then click
     <guimenu>Finish</guimenu>.
    </para>
      </step>
    </procedure>
    <note>
      <title>Time Services on Virtual Guests</title>
      <para>
    Hardware clocks commonly are not very precise. All modern operating
    systems try to correct the system time compared to the hardware time by
    means of an additional time source. To get the correct time on all
    VM Guest systems, also activate the network time services on each
    respective guest or make sure that the guest uses the system time of the
    host. For more about <literal>Independent Wallclocks</literal> in
    <phrase role="productname"><phrase os="osuse">openSUSE</phrase><phrase os="sles">SUSE Linux Enterprise Server</phrase><phrase os="sled">SUSE Linux Enterprise Desktop</phrase><phrase os="slerte">SUSE Linux Enterprise Real Time Extension</phrase></phrase> see <xref linkend="sec.xen.guests.suse.time"/>.
   </para>
    </note>
    <para>
   For more information about managing virtual machines, see
   <xref linkend="cha.xen.manage"/>.
  </para>
  </sect1>
  <sect1 xml:id="sec.xen.vhost.pciback">
    <title>PCI Pass-Through</title>
    <para>
   To take full advantage of VM Guest systems, it is sometimes necessary to
   assign specific PCI devices to a dedicated domain. When using fully
   virtualized guests, this functionality is only available if the chipset
   of the system supports this feature, and if it is activated from the
   BIOS.
  </para>
    <para>
   This feature is available from both AMD* and Intel*. For AMD machines,
   the feature is called <xref linkend="gloss.vt.acronym.iommu"/>; in Intel
   speak, this is <xref linkend="gloss.vt.acronym.vtd"/>. Note that Intel-VT
   technology is not sufficient to use this feature for fully virtualized
   guests. To make sure that your computer supports this feature, ask your
   supplier specifically to deliver a system that supports PCI Pass-Through.
  </para>
    <itemizedlist>
      <title>Limitations</title>
      <listitem>
        <para>
     Some graphics drivers use highly optimized ways to access DMA. This is
     not supported, and thus using graphics cards may be difficult.
    </para>
      </listitem>
      <listitem>
        <para>
     When accessing PCI devices behind a
     <xref linkend="gloss.vt.acronym.pcie"/> bridge, all of the PCI
     devices must be assigned to a single guest. This limitation does not
     apply to <xref linkend="gloss.vt.acronym.pcie"/> devices.
    </para>
      </listitem>
      <listitem>
        <para>
     Guests with dedicated PCI devices cannot be migrated live to a
     different host.
    </para>
      </listitem>
    </itemizedlist>
    <para>
   The configuration of PCI Pass-Through is twofold. First, the hypervisor must be
   informed at boot time that a PCI device should be available for
   reassigning. Second, the PCI device must be assigned to the VM Guest.
  </para>
    <sect2 xml:id="config.hypervisor.pciback">
      <title>Configuring the Hypervisor for PCI Pass-Through</title>
      <procedure>
        <step>
          <para>
      Select a device to reassign to a VM Guest. To do this, run
      <command>lspci</command> and read the device number. For example, if
      <command>lspci</command> contains the following line:
     </para>
          <screen>06:01.0 Ethernet controller: Digital Equipment Corporation DECchip 21142/43 (rev 41)</screen>
          <para>
      In this case, the PCI number is <literal>(06:01.0)</literal>.
     </para>
        </step>
        <step>
          <para>
      Run <menuchoice><guimenu>YaST</guimenu><guimenu>System</guimenu><guimenu>Boot Loader</guimenu></menuchoice>.
     </para>
        </step>
        <step>
          <para>
      Select the <literal>Xen</literal> section and press
      <guimenu>Edit</guimenu>.
     </para>
        </step>
        <step>
          <para>
      Add the PCI number to the <guimenu>Optional Kernel Command Line
      Parameter</guimenu> line:
     </para>
          <screen>pciback.hide=(06:01.0)</screen>
        </step>
        <step>
          <para>
      Press <guimenu>OK</guimenu> and exit YaST.
     </para>
        </step>
        <step>
          <para>
      Reboot the system.
     </para>
        </step>
        <step>
          <para>
      Check if the device is in the list of assignable devices with the
      command
     </para>
          <screen>xl pci-assignable-list</screen>
        </step>
      </procedure>
      <sect3>
        <title>Dynamic Assignment with xl</title>
        <para>
     If you want to avoid restarting the host system, you can use 
     dynamic assignment with xl to use PCI Pass-Through.
    </para>
        <para>
     Begin by making sure that dom0 has the pciback module loaded:
    </para>
        <screen>modprobe pciback</screen>
        <para>
     Then make a device assignable by using <command>xl
     pci-assignable-add</command>. For example, if you wanted to make the
     device <emphasis>06:01.0</emphasis> available for guests, you should
     type the following:
    </para>
        <screen>xl pci-assignable-add 06:01.0</screen>
      </sect3>
    </sect2>
    <sect2>
      <title>Assigning PCI Devices to VM Guest Systems</title>
      <para>
    There are several possibilities to dedicate a PCI device to a VM Guest:
   </para>
      <variablelist>
        <varlistentry>
          <term>Adding the device while installing:</term>
          <listitem>
            <para>
       During installation, add the <literal>pci</literal> line to the
       configuration file:
      </para>
            <screen>pci=['06:01.0']</screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Hotplugging PCI devices to VM Guest systems</term>
          <listitem>
            <para>
       The command <literal>xl</literal> can be used to add or remove PCI
       devices on the fly. To add the device with number
       <literal>06:01.0</literal> to a guest with name
       <literal>sles12</literal> use:
      </para>
            <screen>xl pci-attach sles12 06:01.0</screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Adding the PCI device to Xend</term>
          <listitem>
            <para>
       To add the device to the guest permanently, add the following snippet
       to the guest configuration file:
      </para>
            <screen>pci = [ '06:01.0,power_mgmt=1,permissive=1' ]</screen>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
    After assigning the PCI device to the VM Guest, the guest system must
    care for the configuration and device drivers for this device.
   </para>
    </sect2>
    <sect2>
      <title>VGA Pass-Through</title>
      <para>
    Xen 4.0 and newer supports VGA graphics adapter pass-through on fully
    virtualized VM Guests. The guest can take full control of the graphics
    adapter with high-performance full 3D and video acceleration.
   </para>
      <itemizedlist>
        <title>Limitations</title>
        <listitem>
          <para>
      VGA Pass-Through functionality is similar to PCI Pass-Through and as such also
      requires <xref linkend="gloss.vt.acronym.iommu"/> (or Intel VT-d)
      support from the motherboard chipset and BIOS.
     </para>
        </listitem>
        <listitem>
          <para>
      Only the primary graphics adapter (the one that is used when you power
      on the computer) can be used with VGA Pass-Through.
     </para>
        </listitem>
        <listitem>
          <para>
      VGA Pass-Through is supported only for fully virtualized guests. Paravirtual
      guests (PV) are not supported.
     </para>
        </listitem>
        <listitem>
          <para>
      The graphics card cannot be shared between multiple VM Guests using
      VGA Pass-Through — you can dedicate it to one guest only.
     </para>
        </listitem>
      </itemizedlist>
      <para>
    To enable VGA Pass-Through, add the following settings to your fully
    virtualized guest configuration file:
   </para>
      <screen>gfx_passthru=1 
pci=['yy:zz.n']</screen>
      <para>
    where <literal>yy:zz.n</literal> is the PCI controller ID of the VGA
    graphics adapter as found with <command>lspci -v</command> on Dom0.
   </para>
    </sect2>
    <sect2>
      <title>For More Information</title>
      <para>
    There are several resources on the Internet that provide interesting
    information about PCI Pass-Through:
   </para>
      <itemizedlist>
        <listitem>
          <para>
            <link xlink:href="http://wiki.xensource.com/xenwiki/VTdHowTo"/>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xlink:href="http://software.intel.com/en-us/articles/intel-virtualization-technology-for-directed-io-vt-d-enhancing-intel-platforms-for-efficient-virtualization-of-io-devices/"/>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xlink:href="http://support.amd.com/TechDocs/34434-IOMMU-Rev_1.26_2-11-09.pdf"/>
          </para>
        </listitem>
      </itemizedlist>
    </sect2>
  </sect1>
</chapter>
